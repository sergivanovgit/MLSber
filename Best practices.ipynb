{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предварительная обработка текста сообщений в ЗНО"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# функция удаления имен (список имен берется из файла с именами names.csv)\n",
    "\n",
    "names = pd.read_csv('names.csv')[['name', 'type', 'sex']]\n",
    "\n",
    "def remove_names(string, names=names):\n",
    "    string = ' '+string.lower()+' '\n",
    "    user_name_template = '_user_name_'\n",
    "    type_order = ['f','o','i']\n",
    "    for t in type_order:\n",
    "        all_names = names[names['type']==t]['name'].tolist()\n",
    "        all_types = names[names['type']==t]['type'].tolist()\n",
    "        all_sex = names[names['type']==t]['sex'].tolist()\n",
    "\n",
    "        for i in range(len(all_names)):\n",
    "            name_ = all_names[i]\n",
    "            type_ = all_types[i]\n",
    "            sex_ = all_sex[i]\n",
    "\n",
    "            arr = [name_]\n",
    "            if type_ == 'i' and sex_ == 'm':\n",
    "                if name_[:-2] in string:\n",
    "                    if name_[-2:] in ['ай', 'ей', 'ий']:\n",
    "                        string = re.sub(' '+name_[:-2]+'\\w*вна ', ' '+user_name_template+' ', string)\n",
    "                        string = re.sub(' '+name_[:-2]+'\\w*вич ', ' '+user_name_template+' ', string)\n",
    "                        #arr.extend( re.findall(' '+name_[:-2]+'\\w*вна ', string))\n",
    "                        #arr.extend( re.findall(' '+name_[:-2]+'\\w*вич ', string))\n",
    "                    if name_[-2:] in ['ил']: \n",
    "                        string = re.sub(' '+name_[:-2]+'йл\\w* ', ' '+user_name_template+' ', string)\n",
    "                        string = re.sub(' '+name_[:-2]+'\\w*вна ', ' '+user_name_template+' ', string)\n",
    "                        string = re.sub(' '+name_[:-2]+'\\w*вич ', ' '+user_name_template+' ', string)\n",
    "                        #arr.extend( re.findall(' '+name_[:-2]+'йл\\w* ', string))\n",
    "                        #arr.extend( re.findall(' '+name_[:-2]+'\\w*вна ', string))\n",
    "                        #arr.extend( re.findall(' '+name_[:-2]+'\\w*вич ', string))\n",
    "            if len(name_)>=5:\n",
    "                string = re.sub(' '+name_+'\\w* ', ' '+user_name_template+' ', string)\n",
    "                #arr.extend( re.findall(' '+name_+'\\w* ', string))\n",
    "            if len(name_)>2 and name_ in string:\n",
    "                string = string.replace(' '+name_+' ',' '+user_name_template+' ')\n",
    "\n",
    "            arr = np.unique(arr)\n",
    "            if len(arr)>1 and 1==0:\n",
    "                for name_val in arr:\n",
    "                    string = string.replace(name_val,' '+user_name_template+' ')\n",
    "                    \n",
    "    string = re.sub(' ('+user_name_template+'\\s*)+ ', ' '+user_name_template+' ', string)\n",
    "    return string\n",
    "\n",
    "# функция неразбиения слов с частицами \"не\", \"ни\",\"без\" т.к. это может быть значимо при анализе информации в ЗНО\n",
    "def collapse_ne(string):\n",
    "    string = re.sub(r'\\s+', '  ', string)\n",
    "    string = re.sub(r' не\\s+(\\w+)', r' не\\1', string)\n",
    "    string = re.sub(r' ни\\s+(\\w+)', r' не\\1', string)\n",
    "    string = re.sub(r' без\\s+(\\w+)', r' без\\1', string)\n",
    "    return string\n",
    "\n",
    "# фукция сворачивания массива в строку и преобразование в нижний регистр\n",
    "def to_one_string(arr, is_lower=True):\n",
    "    delimeter = list(set([chr(i).lower() for i in range(255)]) -set(' '.join(arr).lower()) )[-1]\n",
    "    if is_lower:\n",
    "        return delimeter, (' '+(' '+delimeter+' ').join(arr)+' ').lower()\n",
    "    return delimeter, ' '+(' '+delimeter+' ').join(arr)+' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# нормализация текста при помощи MyStem\n",
    "import threading\n",
    "from pymystem3 import Mystem\n",
    "from math import floor\n",
    "semaphore = threading.BoundedSemaphore()\n",
    "m = Mystem()\n",
    "\n",
    "def toMystem(text):\n",
    "#  составление масива слов \n",
    "    arr = text.split()\n",
    "    delimeter, uniq_word_text = to_one_string(list(set(arr)))\n",
    "    print('to_one_string finished. uniq word count is ', len(uniq_word_text.split(delimeter)),' word count is ',(len(arr)))\n",
    "    dict_arr = {}\n",
    "    for i, word in enumerate(arr):\n",
    "        if word not in dict_arr:\n",
    "            dict_arr[word] = []\n",
    "        dict_arr[word].append(i)\n",
    "        \n",
    "# нормализация слов                                              \n",
    "    semaphore.acquire() # decrements the counter\n",
    "    lemmas = m.lemmatize(uniq_word_text)\n",
    "    semaphore.release() # increments the counter\n",
    "    print('m.lemmatize finished')\n",
    "\n",
    "# составление нового массива слов (уже нормализованных)                                              \n",
    "    old_word = uniq_word_text.split(delimeter)\n",
    "    new_word = remove_names(' '.join(lemmas)).split(delimeter)\n",
    "    print('remove_names finished')\n",
    "    new_arr = ['' for i in range(len(arr))]\n",
    "    for i, word in enumerate(old_word):\n",
    "        if i/10000 == floor(i/10000):\n",
    "            print(i)\n",
    "        for j in dict_arr[word.strip()]:\n",
    "            new_arr[j] = new_word[i]\n",
    "        \n",
    "    text = ' '.join(new_arr)\n",
    "    text = collapse_ne(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подборка наилучших параметров алгоритма в модели (на примере LogisticRegression) при помощи GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "              , 'class_weight': [None,'balanced']\n",
    "#              , 'multi_class':['ovr','multinomial']\n",
    "#              ,'solver': ['liblinear','sag']\n",
    "             }\n",
    "#поиск наилучших параметров\n",
    "grid_search = GridSearchCV(LogisticRegression(random_state=9,n_jobs=-1), param_grid, cv=5)\n",
    "grid_search.fit(X_train_ve, y_train)\n",
    "\n",
    "print(\"Правильность на тестовом наборе: {:.2f}\".format(grid_search.score(X_test_ve, y_test)))\n",
    "print(\"Наилучшие значения параметров: {}\".format(grid_search.best_params_))\n",
    "print(\"Наилучшее значение кросс-валидац. правильности: {:.2f}\".format(grid_search.best_score_))\n",
    "print(\"Наилучшая модель:\\n{}\".format(grid_search.best_estimator_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Удаление запросов на группу, которая не подлежит классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.at[data['SM_class'] == 'Ф', 'SM_class'] = 16\n",
    "\n",
    "\n",
    "for (name, series) in data.iterrows():\n",
    "    if data.at[name,'Группа'].count('ОСРСЦентр')==0: data.at[name,'Группа']=np.nan "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Транспонирование коэффициентов классификатора по классам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#clf.coef_ - работает для LogisticRegression\n",
    "#[1, 2, 3, 4] - список классов, на которые распределяет наш классификатор\n",
    "dd = pd.DataFrame(data=np.transpose(clf.coef_), index=list(vectorizer.get_feature_names()), columns=[1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Класс, описывающий классификатор по принципу \"один против всех\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "class One_vs_all():\n",
    "    def __init__(self, train, y_train, random_state, params, regressor, vectorizer\n",
    "                 , num_boost_round = 500):\n",
    "        self.train = train\n",
    "        self.y_train = y_train\n",
    "        self.random_state = random_state\n",
    "        self.params = params\n",
    "        self.num_boost_round = num_boost_round\n",
    "        self.y_dummied = ''\n",
    "        self.one_vs_all_models = {}\n",
    "        \n",
    "        self.vectorizer = vectorizer\n",
    "        self.regressor = regressor\n",
    "        self.is_preprocessed = False\n",
    "        self.cv_dict = {}\n",
    "    \n",
    "    def remove_zero_rows(self,matr):\n",
    "        nonzero_row_indice, _ = matr.nonzero()\n",
    "        unique_nonzero_indice = np.unique(nonzero_row_indice)\n",
    "        return matr[unique_nonzero_indice], unique_nonzero_indice\n",
    "\n",
    "    def preprocess_fit(self):\n",
    "        if self.is_preprocessed:\n",
    "            return\n",
    "        self.vectorizer = self.vectorizer.fit(self.train)\n",
    "        train_tfidf = self.vectorizer.transform(self.train)\n",
    "        \n",
    "        self.train_tfidf, not_zero_rows = self.remove_zero_rows(train_tfidf)\n",
    "        \n",
    "        self.y_train= self.y_train.iloc[not_zero_rows]\n",
    "\n",
    "        self.y_dummied = pd.get_dummies(self.y_train)\n",
    "        for class_name in self.y_dummied.columns:\n",
    "            if self.y_dummied[class_name].sum() < 10:\n",
    "                self.y_dummied.drop(class_name, axis=1, inplace=True)\n",
    "                \n",
    "        self.is_preprocessed = True\n",
    "    \n",
    "    def model_fit(self):\n",
    "        for class_name in self.y_dummied.columns:\n",
    "            y_train = self.y_dummied[class_name]\n",
    "            temp_params = self.params\n",
    "            scale = len(y_train)/y_train.sum() \n",
    "            if self.regressor == xgb:\n",
    "                dtrain = xgb.DMatrix(self.train_tfidf, y_train)\n",
    "                temp_params[\"scale_pos_weight\"] = scale\n",
    "                class_model = xgb.train(temp_params, dtrain, self.num_boost_round)\n",
    "            else:\n",
    "                dtrain = self.train_tfidf\n",
    "                params = self.params\n",
    "                params['random_state'] = self.random_state\n",
    "                class_model = self.regressor(**params).fit(dtrain, y_train)\n",
    "            \n",
    "            self.one_vs_all_models[class_name] = class_model\n",
    "        #del self.train_tfidf\n",
    "        #del self.y_dummied\n",
    "    \n",
    "    def cv_model(self):\n",
    "        self.preprocess_fit()\n",
    "        if len(self.cv_dict) > 0:\n",
    "            return self.cv_dict\n",
    "        \n",
    "        cv_dict = {}        \n",
    "        #sss = StratifiedShuffleSplit(n_splits=5, test_size=0.30)\n",
    "        for class_name in self.y_dummied.columns:\n",
    "            print('cv for ',class_name)\n",
    "            y_train = self.y_dummied[class_name]\n",
    "            if self.regressor == xgb:\n",
    "                dtrain = xgb.DMatrix(self.train_tfidf, y_train)\n",
    "                cv_info = xgboost.cv(self.params, dtrain, num_boost_round=self.num_boost_round, nfold=5, stratified=True,metrics='logloss',verbose_eval=100)\n",
    "                '''else:\n",
    "                params = self.params\n",
    "                params['random_state'] = self.random_state\n",
    "                scoresSSS = cross_val_score(self.regressor(**params), self.train_tfidf, y_train, cv=5, n_jobs=-1)\n",
    "                cv_info = np.average(scoresSSS)\n",
    "                #print(cv_info)'''\n",
    "            cv_dict[class_name] = cv_info\n",
    "        #del self.train_tfidf\n",
    "        #del self.y_dummied\n",
    "        self.cv_dict = cv_dict\n",
    "        return cv_dict\n",
    "    \n",
    "    def clear(self):\n",
    "        del self.train_tfidf\n",
    "        del self.y_dummied\n",
    "        \n",
    "    def predict_single(self, text):\n",
    "        test = [text]\n",
    "        test_tfidf = self.vectorizer.transform(test)\n",
    "        if self.regressor == xgb:\n",
    "            dtest = xgb.DMatrix(test_tfidf)\n",
    "        else:\n",
    "            dtest = test_tfidf\n",
    "        result = {}\n",
    "        if type(self.regressor).__name__ in ['type', 'module']:\n",
    "            for class_name in self.one_vs_all_models:\n",
    "                model = self.one_vs_all_models[class_name]\n",
    "                proba = model.predict(dtest)[0].apply(str)\n",
    "                if proba > 0.5:\n",
    "                    result[class_name] = proba\n",
    "        else:\n",
    "            result['result'] = str(self.regressor.predict(dtest)[0])\n",
    "        return result\n",
    "    \n",
    "    def predict(self, test):\n",
    "        #print(type(test).__name__, test)\n",
    "        if isinstance(test, str):\n",
    "            return self.predict_single(test)\n",
    "            \n",
    "        test_tfidf = self.vectorizer.transform(test)\n",
    "        if self.regressor == xgb:\n",
    "            dtest = xgb.DMatrix(test_tfidf)\n",
    "        \n",
    "        if isinstance(test, pd.Series):\n",
    "            index = test.index\n",
    "            result = pd.DataFrame(columns = self.one_vs_all_models.keys(), index=index)\n",
    "        else:\n",
    "            result = pd.DataFrame(columns = self.one_vs_all_models.keys())\n",
    "        #result = pd.DataFrame()\n",
    "        \n",
    "        if type(self.regressor).__name__ in ['type', 'module']:\n",
    "            for class_name in self.one_vs_all_models:\n",
    "                model = self.one_vs_all_models[class_name]\n",
    "                if self.regressor == xgb:\n",
    "                    proba = model.predict(dtest)\n",
    "                else:\n",
    "                    proba = model.predict(test_tfidf)\n",
    "                result[class_name.apply(str)] = proba.apply(str)\n",
    "            return result.idxmax(axis=1).apply(str), result.apply(str)\n",
    "        else:\n",
    "            result['result'] = self.regressor.predict(test_tfidf)\n",
    "            return result.max(axis=1).apply(str), result.apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# формирование модели по принципу \"один против всех\"\n",
    "clf_model = One_vs_all(train=None, y_train=None, random_state=42, params={}, regressor=clf, vectorizer=vectorizer)\n",
    "\n",
    "# предсказание данных \n",
    "y_pred, result=clf_model.predict(X_test)\n",
    "# вывод процента точности\n",
    "print(\"Правильность на тестовом наборе: {:.2f}\".format(np.mean(y_pred == y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
